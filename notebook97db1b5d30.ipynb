{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-phone",
   "metadata": {
    "papermill": {
     "duration": 0.020454,
     "end_time": "2021-06-21T13:59:29.925648",
     "exception": false,
     "start_time": "2021-06-21T13:59:29.905194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#COMMON READABILITY COMPETITION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-lewis",
   "metadata": {
    "papermill": {
     "duration": 0.022605,
     "end_time": "2021-06-21T13:59:29.969948",
     "exception": false,
     "start_time": "2021-06-21T13:59:29.947343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean the Unstructured Language Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-delhi",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T13:59:30.019801Z",
     "iopub.status.busy": "2021-06-21T13:59:30.018943Z",
     "iopub.status.idle": "2021-06-21T13:59:30.022917Z",
     "shell.execute_reply": "2021-06-21T13:59:30.022114Z",
     "shell.execute_reply.started": "2021-06-21T13:38:12.432402Z"
    },
    "papermill": {
     "duration": 0.032838,
     "end_time": "2021-06-21T13:59:30.023238",
     "exception": false,
     "start_time": "2021-06-21T13:59:29.990400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#*Importing Main Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "northern-attack",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T13:59:30.063807Z",
     "iopub.status.busy": "2021-06-21T13:59:30.063073Z",
     "iopub.status.idle": "2021-06-21T13:59:30.204068Z",
     "shell.execute_reply": "2021-06-21T13:59:30.204867Z",
     "shell.execute_reply.started": "2021-06-21T13:38:12.795233Z"
    },
    "papermill": {
     "duration": 0.163352,
     "end_time": "2021-06-21T13:59:30.205102",
     "exception": false,
     "start_time": "2021-06-21T13:59:30.041750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "training_syntax= pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "testing_syntax=pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "save=pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-michael",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T13:59:30.244030Z",
     "iopub.status.busy": "2021-06-21T13:59:30.243376Z",
     "iopub.status.idle": "2021-06-21T13:59:30.274855Z",
     "shell.execute_reply": "2021-06-21T13:59:30.274320Z",
     "shell.execute_reply.started": "2021-06-21T13:38:13.474496Z"
    },
    "papermill": {
     "duration": 0.050564,
     "end_time": "2021-06-21T13:59:30.274991",
     "exception": false,
     "start_time": "2021-06-21T13:59:30.224427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training_syntax (2834, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "2831  0.255209        0.483866  \n",
       "2832 -0.215279        0.514128  \n",
       "2833  0.300779        0.512379  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore the data: general shape od the df\n",
    "print(\"shape of training_syntax\",training_syntax.shape)\n",
    "training_syntax.head(3)\n",
    "training_syntax.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-genealogy",
   "metadata": {
    "papermill": {
     "duration": 0.016766,
     "end_time": "2021-06-21T13:59:30.309163",
     "exception": false,
     "start_time": "2021-06-21T13:59:30.292397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Clean and turn the Unstructured Language Data to structured data structures(array, dictionary, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defined-burlington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T13:59:30.357189Z",
     "iopub.status.busy": "2021-06-21T13:59:30.356528Z",
     "iopub.status.idle": "2021-06-21T14:01:06.599786Z",
     "shell.execute_reply": "2021-06-21T14:01:06.599204Z",
     "shell.execute_reply.started": "2021-06-21T13:38:15.132017Z"
    },
    "papermill": {
     "duration": 96.273708,
     "end_time": "2021-06-21T14:01:06.599957",
     "exception": false,
     "start_time": "2021-06-21T13:59:30.326249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "#Importing NLTK librariries so that to clean the unstructured language data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "english_spacy = spacy.load('en_core_web_lg')\n",
    "from spacy import displacy\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#removing punctuations and stopwords from unstructured langugae data \n",
    "def stopwordsPunctuation_detacher(syntax):\n",
    "    syntax = re.sub(r'[^\\w\\s]','',syntax)\n",
    "    syntax = [lexUnit.lower() for lexUnit in syntax.lower().split() if lexUnit not in stopwords]\n",
    "    syntax=' '.join(syntax)\n",
    "    syntax.translate(str.maketrans('','', string.punctuation))\n",
    "    english_sytax = english_spacy(syntax)\n",
    "    new_syntax = ' '\n",
    "    absence=0\n",
    "    for lex_unit in english_sytax:\n",
    "        if (lex_unit.is_stop == absence):\n",
    "            new_syntax = new_syntax + ' ' + str(lex_unit)\n",
    "    return new_syntax\n",
    "\n",
    "#removing digits, webLinks from unstructured langugae data and making all words lower case.\n",
    "def textCleaning_engine(syntax):\n",
    "    syntax = syntax.lower()\n",
    "    syntax = re.sub('\\[.*?\\]', '', syntax)\n",
    "    syntax = re.sub('https?://\\S+|www\\.\\S+', '', syntax)\n",
    "    syntax = re.sub('<.*?>+', '', syntax)\n",
    "    syntax = re.sub('[%s]' % re.escape(string.punctuation), '', syntax)\n",
    "    syntax = re.sub('\\n', '', syntax)\n",
    "    syntax = re.sub('\\w*\\d\\w*', '', syntax)\n",
    "    clean_syntax = re.sub(r'\\d+','',syntax)\n",
    "    return clean_syntax\n",
    "#Combining the two functions above in one function\n",
    "def ready_sytaxt(syntax):\n",
    "    syntax = syntax.lower()\n",
    "    syntax = stopwordsPunctuation_detacher(syntax)\n",
    "    syntax = textCleaning_engine(syntax)\n",
    "    return syntax\n",
    "#applying the function created above to the datasets so that to remove the unecessary lexical units(such as stopWOrds, punctuations, digits, links, ets)\n",
    "training_syntax['excerpt_clean'] = training_syntax['excerpt'].apply(ready_sytaxt)\n",
    "testing_syntax['excerpt_clean'] = testing_syntax['excerpt'].apply(ready_sytaxt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turkish-creation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:06.666334Z",
     "iopub.status.busy": "2021-06-21T14:01:06.660999Z",
     "iopub.status.idle": "2021-06-21T14:01:10.542461Z",
     "shell.execute_reply": "2021-06-21T14:01:10.541627Z",
     "shell.execute_reply.started": "2021-06-21T13:39:33.840176Z"
    },
    "papermill": {
     "duration": 3.924236,
     "end_time": "2021-06-21T14:01:10.542616",
     "exception": false,
     "start_time": "2021-06-21T14:01:06.618380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of the stem of the words within the text/ for debug purpose:  dinner-->dinner\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  time-->time\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  mrs-->mrs\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  fayre-->fayr\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  somewhat-->somewhat\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  silent-->silent\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  eyes-->eye\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  resting-->rest\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  dolly-->dolli\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  wistful-->wist\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  uncertain-->uncertain\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  expression-->express\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  wanted-->want\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  child-->child\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  pleasure-->pleasur\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  craved-->crave\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  hard-->hard\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  work-->work\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  bring-->bring\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  point-->point\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  overcoming-->overcom\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  objections-->object\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  meal-->meal\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  nearly-->near\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  smiled-->smile\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  little-->littl\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  daughter-->daughter\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  said-->said\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  right-->right\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  dolly-->dolli\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  oh-->oh\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  mother-->mother\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  dolly-->dolli\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  cried-->cri\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  overwhelmed-->overwhelm\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  sudden-->sudden\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  delight-->delight\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  oh-->oh\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  glad-->glad\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  sure-->sure\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  willing-->will\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  ve-->ve\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  persuaded-->persuad\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  willing-->will\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  returned-->return\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  mrs-->mrs\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  fayre-->fayr\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  whimsically-->whimsic\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  confess-->confess\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  hate-->hate\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  nt-->nt\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  bear-->bear\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  deprive-->depriv\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  pleasure-->pleasur\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  trip-->trip\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  dotty-->dotti\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  home-->home\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  altogether-->altogeth\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  think-->think\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  shall-->shall\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  oh-->oh\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  angel-->angel\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  mother-->mother\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  blessed-->bless\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  lady-->ladi\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  good-->good\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  dolly-->dolli\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  flew-->flew\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  table-->tabl\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  gave-->gave\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  mother-->mother\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  hug-->hug\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  nearly-->near\n",
      "This is an example of the stem of the words within the text/ for debug purpose:  suffocated-->suffoc\n",
      "\n",
      "\n",
      "Dictionary Keys Container 28830\n",
      "This is an example of clean texts for debug purpose:  ------> 0     young peopl return ballroom present decid cha...\n",
      "1     dinner time mrs fayr somewhat silent eye rest...\n",
      "2     roger predict snow depart quick came day slei...\n",
      "3     outsid palac great garden wall round fill sta...\n",
      "4     time bear live hous wood littl small wee bear...\n",
      "Name: excerpt_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#storing the elements of training_syntax['excerpt_clean'] in a new container as dictionary keys and those elements' counts as dictionary values.\n",
    "from collections import Counter\n",
    "dictionaryKeys_container = Counter()\n",
    "training_syntax['excerpt_clean'].str.lower().str.split().apply(dictionaryKeys_container.update)\n",
    "\n",
    "# reducing a word to its word stem(roots of words) to determine domain vocabularies in domain analysis\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer = SnowballStemmer(language = 'english')\n",
    "stem_list = training_syntax['excerpt_clean'][1].split()\n",
    "new_syntax = ''\n",
    "for stem in stem_list:\n",
    "    print(\"This is an example of the stem of the words within the text/ for debug purpose:  \"+ stem + '-->' + SnowballStemmer.stem(stem)) \n",
    "    \n",
    "#create a function to split the language units in the text data and extract their stem\n",
    "def stemOf_lexUnit(syntax):\n",
    "    snowball_stemmer = SnowballStemmer(language='english')\n",
    "    token_list = syntax.split()\n",
    "    new_syntax = ''\n",
    "    for token_unit in token_list:\n",
    "        new_syntax = new_syntax + ' ' + snowball_stemmer.stem(token_unit)\n",
    "    return new_syntax\n",
    "\n",
    "#apply the created function above to extract the stem of the lexical units in the unstructured language data\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "training_syntax['excerpt_clean'] = training_syntax['excerpt_clean'].apply(stemOf_lexUnit)\n",
    "testing_syntax['excerpt_clean'] = testing_syntax['excerpt_clean'].apply(stemOf_lexUnit)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Dictionary Keys Container\", len(dictionaryKeys_container.keys()))\n",
    "print(\"This is an example of clean texts for debug purpose: \",\"------>\",training_syntax['excerpt_clean'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-ballet",
   "metadata": {
    "papermill": {
     "duration": 0.018237,
     "end_time": "2021-06-21T14:01:10.579300",
     "exception": false,
     "start_time": "2021-06-21T14:01:10.561063",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-ethnic",
   "metadata": {
    "papermill": {
     "duration": 0.018296,
     "end_time": "2021-06-21T14:01:10.616269",
     "exception": false,
     "start_time": "2021-06-21T14:01:10.597973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Assigning parameters and data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "higher-spell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:10.661110Z",
     "iopub.status.busy": "2021-06-21T14:01:10.660445Z",
     "iopub.status.idle": "2021-06-21T14:01:10.665609Z",
     "shell.execute_reply": "2021-06-21T14:01:10.665081Z",
     "shell.execute_reply.started": "2021-06-21T13:42:15.083073Z"
    },
    "papermill": {
     "duration": 0.030951,
     "end_time": "2021-06-21T14:01:10.665755",
     "exception": false,
     "start_time": "2021-06-21T14:01:10.634804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X_excerpt_clean:  2834 \n",
      "       length of y_target:  2834\n",
      "length of test_trained_X:   7\n",
      "length of syntax:           2834\n"
     ]
    }
   ],
   "source": [
    "#define X and y features\n",
    "X = training_syntax['excerpt_clean']\n",
    "y = training_syntax['target']\n",
    "print(\"length of X_excerpt_clean: \", len(X),'\\n      ',\"length of y_target: \", len(y))\n",
    "\n",
    "#assign a variable for each of the test and train data\n",
    "test_trained_X = testing_syntax['excerpt_clean']\n",
    "print(\"length of test_trained_X:  \",len(test_trained_X))\n",
    "syntax = training_syntax['excerpt_clean']\n",
    "print(\"length of syntax:          \",len(syntax))\n",
    "\n",
    "#define parametres\n",
    "lexicalUnit_quantity = 16662\n",
    "attaching_dimention = 64\n",
    "max_length = 50\n",
    "trimming_type = 'post'\n",
    "packing_type = 'post'\n",
    "fill_gap = '<OOV>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-nightmare",
   "metadata": {
    "papermill": {
     "duration": 0.018468,
     "end_time": "2021-06-21T14:01:10.704460",
     "exception": false,
     "start_time": "2021-06-21T14:01:10.685992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vectorize a text corpus and create pad sequences for both train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compliant-discretion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:10.749922Z",
     "iopub.status.busy": "2021-06-21T14:01:10.749268Z",
     "iopub.status.idle": "2021-06-21T14:01:16.254475Z",
     "shell.execute_reply": "2021-06-21T14:01:16.253871Z",
     "shell.execute_reply.started": "2021-06-21T13:42:17.412062Z"
    },
    "papermill": {
     "duration": 5.531143,
     "end_time": "2021-06-21T14:01:16.254617",
     "exception": false,
     "start_time": "2021-06-21T14:01:10.723474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pad seuqneces of training set:   \n",
      " [[   75     7   189 ...   102    96   270]\n",
      " [  576     4   319 ... 10490  2637  2638]\n",
      " [ 3097  1491   553 ...   777     5   123]\n",
      " ...\n",
      " [  177    37   131 ...   330  1870   739]\n",
      " [  637   443   585 ...    26  1419   320]\n",
      " [   60    73   121 ...     5  1487  1061]] \n",
      "\n",
      "\n",
      "This is a shape of the pad seuqneces of training set:   \n",
      " (2834, 50)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#vectorize a text corpus, by turning each text into either a sequence of integers\n",
    "tokenize_engine = Tokenizer(num_words = lexicalUnit_quantity,\n",
    "                            oov_token=fill_gap)\n",
    "#Update internal vocabulary based on a list of sequences.\n",
    "tokenize_engine.fit_on_texts(syntax)\n",
    "#named list mapping words to their rank/index (int)\n",
    "lexUnit_index = tokenize_engine.word_index\n",
    "\n",
    "#Transform each text in texts to a sequence of integers.\n",
    "textsToSequences_train = tokenize_engine.texts_to_sequences(syntax)\n",
    "#Making all sequences in a list have the same length: By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence\n",
    "pad_sequences = pad_sequences(textsToSequences_train, \n",
    "                              maxlen = max_length,\n",
    "                              truncating=trimming_type,\n",
    "                              padding = packing_type)\n",
    "#Creating target label converting the training set into an array \n",
    "targetLabel = np.array(training_syntax['target'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"This is pad seuqneces of training set:  \",'\\n', pad_sequences, '\\n\\n') \n",
    "print(\"This is a shape of the pad seuqneces of training set:  \",'\\n',pad_sequences.shape)\n",
    "# print(\"This is a shape of the pad seuqneces of testing set:  \",'\\n, pad_sequences_test.shape )\n",
    "print('\\n') \n",
    "                           ##############                  ##############                  ##############                  ##############                  ##############                  \n",
    "# textsToSequences_testing = tokenize_engine.texts_to_sequences(testing_syntax['excerpt_clean'])\n",
    "# pad_sequences_test = pad_sequences(textsToSequences_testing, \n",
    "#                                    maxlen = max_length, \n",
    "#                                    trimming_type='post',\n",
    "#                                    padding = packing_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-bulletin",
   "metadata": {
    "papermill": {
     "duration": 0.018891,
     "end_time": "2021-06-21T14:01:16.292629",
     "exception": false,
     "start_time": "2021-06-21T14:01:16.273738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Applying LinearRegression & GradientBoostingRegressor MODELS of ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "russian-majority",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:16.340091Z",
     "iopub.status.busy": "2021-06-21T14:01:16.339471Z",
     "iopub.status.idle": "2021-06-21T14:01:17.340253Z",
     "shell.execute_reply": "2021-06-21T14:01:17.340777Z",
     "shell.execute_reply.started": "2021-06-21T13:42:29.792517Z"
    },
    "papermill": {
     "duration": 1.029101,
     "end_time": "2021-06-21T14:01:17.340948",
     "exception": false,
     "start_time": "2021-06-21T14:01:16.311847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.854993120472505\n",
      "Root Mean Squared Error: 0.9246583804154402\n",
      "\n",
      "\n",
      "Mean Squared Error: 0.7799577599942297\n",
      "Root Mean Squared Error: 0.8831521726147933\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################################____LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MSE=\"Mean Squared Error:\"\n",
    "RMSE=\"Root Mean Squared Error:\"\n",
    "\n",
    "#split the train and test data sets as categorizing them as traina nd validation types for each x and Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_validation,y_train,y_validation = train_test_split(pad_sequences,\n",
    "                                                             targetLabel,\n",
    "                                                             random_state = 42)\n",
    "#Let's model the relationship between two variables by fitting(calculating an equation that minimizes the distance between the fitted line and all of the data points) a linear equation to observed data\n",
    "model_LinearRegression = LinearRegression()\n",
    "model_LinearRegression.fit(X_train,y_train)\n",
    "#predict (the average) Y data from X data\n",
    "y_prediction = model_LinearRegression.predict(X_validation)\n",
    "print(MSE, mse(y_validation,\n",
    "               y_prediction))\n",
    "print(RMSE, np.sqrt(mse(y_validation,\n",
    "                        y_prediction)))\n",
    "print('\\n')\n",
    "#optimization of arbitrary differentiable loss functions: additive model in a forward stage-wise way where in each stage a regression tree is fit on the negative gradient of the given loss function\n",
    "modelMain_GradientBoostingRegressor = GradientBoostingRegressor(random_state = 42, \n",
    "                                                                max_depth = 13, \n",
    "                                                                max_features = 19,\n",
    "                                                                min_samples_split = 600, \n",
    "                                                                n_estimators = 70,\n",
    "                                                                subsample=0.75)\n",
    "#use fit function of the GradientBoostingRegressor so that to adjust weights according to data values so that better accuracy can be achieved.\n",
    "modelMain_GradientBoostingRegressor.fit(X_train,y_train)\n",
    "y_prediction = modelMain_GradientBoostingRegressor.predict(X_validation)\n",
    "print(MSE, mse(y_validation,\n",
    "               y_prediction))\n",
    "print(RMSE, np.sqrt(mse(y_validation,\n",
    "                        y_prediction)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-armor",
   "metadata": {
    "papermill": {
     "duration": 0.019622,
     "end_time": "2021-06-21T14:01:17.380320",
     "exception": false,
     "start_time": "2021-06-21T14:01:17.360698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Obtaining vector representations of words(using GloVe) and create a vector of the syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jewish-priority",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:17.422882Z",
     "iopub.status.busy": "2021-06-21T14:01:17.422217Z",
     "iopub.status.idle": "2021-06-21T14:01:30.072102Z",
     "shell.execute_reply": "2021-06-21T14:01:30.071537Z",
     "shell.execute_reply.started": "2021-06-21T13:42:39.645287Z"
    },
    "papermill": {
     "duration": 12.672324,
     "end_time": "2021-06-21T14:01:30.072266",
     "exception": false,
     "start_time": "2021-06-21T14:01:17.399942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a debug msg to check the index of the specific word \n",
      " index num of thie word is:  (50,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying multilayered bidirectional LSTM to classify text and GloVe embeddings for words(rather than random embeddings)\n",
    "#GloVe is an unsupervised learning algorithm for obtaining vector representations for words(Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.)\n",
    "attaching_index = {}\n",
    "GLoVE_LSTM_dir = '../input/glove6b50dtxt/glove.6B.50d.txt'\n",
    "GLoVE_LSTM_dir = open(GLoVE_LSTM_dir)\n",
    "for row in GLoVE_LSTM_dir:\n",
    "    split_row = row.split()\n",
    "    lexUnit = split_row[0]\n",
    "    coefficients= np.asarray(split_row[1:], dtype='float32')\n",
    "    attaching_index[lexUnit] = coefficients\n",
    "GLoVE_LSTM_dir.close()\n",
    "print(\"this is a debug msg to check the index of the specific word\", \"\\n\", \"index num of thie word is: \", attaching_index['apple'].shape)\n",
    "print('\\n')\n",
    "\n",
    "#Creating grid of values(all of the same type) by the help of np.array and multidimensional container of items of the same type and size by the help of np.ndarray.\n",
    "#sum of vectors are devided by squareRoot\n",
    "def syntaxVector(s):\n",
    "    lexUnit_bag = str(s).lower()\n",
    "    lexUnit_bag = word_tokenize(lexUnit_bag)\n",
    "    lexUnit_bag = [lexUnit for lexUnit in lexUnit_bag if not lexUnit in stop_words]\n",
    "    lexUnit_bag = [lexUnit for lexUnit in lexUnit_bag if lexUnit.isalpha()]\n",
    "    vector = []\n",
    "    for lexUnit in lexUnit_bag:\n",
    "        try:\n",
    "            vector.append(attaching_index[lexUnit])\n",
    "        except:\n",
    "            continue\n",
    "    vector = np.array(vector)\n",
    "    vector_sum = vector.sum(axis=0)\n",
    "    if type(vector_sum) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return vector_sum / np.sqrt((vector_sum ** 2).sum())\n",
    "\n",
    "#split the train and test data sets as categorizing them as train and validation types for each x and Y\n",
    "X_train,X_validation,y_train,y_validation = train_test_split(training_syntax['excerpt_clean'],\n",
    "                                                             training_syntax['target'],\n",
    "                                                             test_size = 0.2,random_state = 35)\n",
    "#Loop through each of the train and validation categories of X data\n",
    "vectorOf_X_trian = [syntaxVector(trainUnit) for trainUnit in X_train]\n",
    "vectorOf_X_validation = [syntaxVector(validationUnit) for validationUnit in X_validation]\n",
    "vectorOf_X_test = [syntaxVector(testUnit) for testUnit in test_trained_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-classroom",
   "metadata": {
    "papermill": {
     "duration": 0.021031,
     "end_time": "2021-06-21T14:01:30.113568",
     "exception": false,
     "start_time": "2021-06-21T14:01:30.092537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Searching hyperParametres in a randomized manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "working-marine",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-06-21T14:01:30.166438Z",
     "iopub.status.busy": "2021-06-21T14:01:30.165786Z",
     "iopub.status.idle": "2021-06-21T14:04:53.238264Z",
     "shell.execute_reply": "2021-06-21T14:04:53.238951Z",
     "shell.execute_reply.started": "2021-06-21T13:43:02.122116Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 203.105331,
     "end_time": "2021-06-21T14:04:53.239201",
     "exception": false,
     "start_time": "2021-06-21T14:01:30.133870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 10 candidates, totalling 150 fits\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=13, max_features=17, min_samples_split=480, n_estimators=120, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=180, n_estimators=60, subsample=0.75; total time=   1.0s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.5s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=19, max_features=14, min_samples_split=180, n_estimators=100, subsample=0.7; total time=   1.6s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.2s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.3s\n",
      "[CV] END max_depth=16, max_features=15, min_samples_split=480, n_estimators=120, subsample=0.9; total time=   2.1s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=7, max_features=16, min_samples_split=180, n_estimators=120, subsample=0.75; total time=   1.7s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.6s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.6s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.6s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.6s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.6s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=19, max_features=11, min_samples_split=780, n_estimators=100, subsample=0.7; total time=   0.5s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   1.0s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=16, max_features=8, min_samples_split=480, n_estimators=140, subsample=0.7; total time=   0.9s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END max_depth=13, max_features=15, min_samples_split=480, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.7s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.9s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.9s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.9s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.7s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.9s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.9s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=16, max_features=13, min_samples_split=480, n_estimators=140, subsample=0.8; total time=   1.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.7s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.7s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.7s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.7s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "[CV] END max_depth=13, max_features=14, min_samples_split=480, n_estimators=60, subsample=0.9; total time=   0.8s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These are the best solution for my model:  subsample\n",
      "These are the best solution for my model:  n_estimators\n",
      "These are the best solution for my model:  min_samples_split\n",
      "These are the best solution for my model:  max_features\n",
      "These are the best solution for my model:  max_depth\n",
      "\n",
      "\n",
      "These are the best estimetor for my model:  GradientBoostingRegressor(max_depth=16, max_features=8, min_samples_split=480,\n",
      "                          n_estimators=140, subsample=0.7)\n",
      "\n",
      "\n",
      "Mean Squared Error: 0.6607244636968218\n",
      "Root Mean Squared Error: 0.8128495947571247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#define a model\n",
    "model_GradientBoostingRegressor= GradientBoostingRegressor()\n",
    "#assign parametres for RandomizedSearchCV\n",
    "parametres_GBR = {'n_estimators': range(40,160,20), \n",
    "                  'max_depth':range(7,22,3), \n",
    "                  'min_samples_split':range(180,1020,300),\n",
    "                  'max_features':range(5,18,1),\n",
    "                  'subsample':[0.9,0.8,0.80,0.7,0.75,0.7]}\n",
    "\n",
    "#Searching hyperParametres in a randomized manner implementing fit method \n",
    "RandomizedSearch_CV = RandomizedSearchCV(model_GradientBoostingRegressor,\n",
    "                                         parametres_GBR, cv = 15,\n",
    "                                         scoring = 'neg_mean_squared_error',verbose=2)\n",
    "RandomizedSearch_CV.fit(vectorOf_X_trian,y_train)\n",
    "#Searching hyperParametres in a randomized manner implementing score method\n",
    "RandomizedSearch_CV1 = RandomizedSearchCV(model_GradientBoostingRegressor,\n",
    "                                          parametres_GBR, \n",
    "                                          cv = 5,\n",
    "                                          scoring = 'neg_mean_squared_error',\n",
    "                                          verbose=1)\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "for i in RandomizedSearch_CV.best_params_:\n",
    "    print(\"These are the best solution for my model: \",i)#The drawback of random search(compared ith gridSearch) is that it yields high variance during computing.\n",
    "print('\\n')\n",
    "print(\"These are the best estimetor for my model: \",RandomizedSearch_CV.best_estimator_)\n",
    "print('\\n')\n",
    "\n",
    "finalModel_RandomizedSearch_CV = RandomizedSearch_CV.best_estimator_\n",
    "finalModel_RandomizedSearch_CV.fit(vectorOf_X_trian,y_train)\n",
    "y_prediction = finalModel_RandomizedSearch_CV.predict(vectorOf_X_validation)\n",
    "print(MSE, mse(y_validation,y_prediction))\n",
    "print(RMSE, np.sqrt(mse(y_validation,y_prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "simple-finish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:04:53.375712Z",
     "iopub.status.busy": "2021-06-21T14:04:53.375032Z",
     "iopub.status.idle": "2021-06-21T14:04:53.382723Z",
     "shell.execute_reply": "2021-06-21T14:04:53.383220Z",
     "shell.execute_reply.started": "2021-06-21T13:46:50.783172Z"
    },
    "papermill": {
     "duration": 0.078686,
     "end_time": "2021-06-21T14:04:53.383409",
     "exception": false,
     "start_time": "2021-06-21T14:04:53.304723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create submission for kaggle\n",
    "y_prediction=pd.DataFrame(y_prediction) \n",
    "# finalModel_RandomizedSearch_CV=pd.DataFrame(finalModel_RandomizedSearch_CV) \n",
    "# #Save submission to CSV\n",
    "y_prediction.to_csv('submission.csv', index=False)  \n",
    "# finalModel_RandomizedSearch_CV.to_csv('finalModel_RandomizedSearch_CV.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-toddler",
   "metadata": {
    "papermill": {
     "duration": 0.064865,
     "end_time": "2021-06-21T14:04:53.513499",
     "exception": false,
     "start_time": "2021-06-21T14:04:53.448634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I would appreciate your upwote. Thanks in advance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-creator",
   "metadata": {
    "papermill": {
     "duration": 0.065074,
     "end_time": "2021-06-21T14:04:53.643525",
     "exception": false,
     "start_time": "2021-06-21T14:04:53.578451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you have further questions related to this project, please feel free to contact with me. Email: hosiyatsabirova@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-suicide",
   "metadata": {
    "papermill": {
     "duration": 0.064908,
     "end_time": "2021-06-21T14:04:53.773771",
     "exception": false,
     "start_time": "2021-06-21T14:04:53.708863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-virtue",
   "metadata": {
    "papermill": {
     "duration": 0.066326,
     "end_time": "2021-06-21T14:04:53.905075",
     "exception": false,
     "start_time": "2021-06-21T14:04:53.838749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 334.949396,
   "end_time": "2021-06-21T14:04:57.070557",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-21T13:59:22.121161",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
